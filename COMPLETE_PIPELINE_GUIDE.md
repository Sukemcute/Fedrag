# HÆ°á»›ng Dáº«n ToÃ n Bá»™ Pipeline FedE4RAG: Tá»« Upstream Äáº¿n Downstream

## ğŸ“‹ Tá»•ng Quan

FedE4RAG lÃ  má»™t framework sá»­ dá»¥ng Federated Learning Ä‘á»ƒ fine-tune embedding models cho RAG (Retrieval-Augmented Generation) systems. Pipeline bao gá»“m 2 pháº§n chÃ­nh:

1. **Upstream (Federated Learning Training)**: Fine-tune embedding models sá»­ dá»¥ng federated learning
2. **Downstream (RAG Evaluation)**: ÄÃ¡nh giÃ¡ cÃ¡c embedding models Ä‘Ã£ fine-tune trong RAG pipeline

---

## âš¡ TÃ³m Táº¯t Nhanh CÃ¡c BÆ°á»›c

### Upstream (Federated Learning)
```bash
# 1. CÃ i Miniconda vÃ  táº¡o environment
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
bash Miniconda3-latest-Linux-x86_64.sh
conda create -n fedrag python=3.11 -y
conda activate fedrag

# 2. Clone repo vÃ  cÃ i dependencies
git clone https://github.com/Sukemcute/Fedrag.git
cd Fedrag/FedE
pip install -r requirements.txt
pip install transformers==4.35.0
pip install "numpy<2"

# 3. Cháº¡y training
bash run.sh

# 4. Model sáº½ Ä‘Æ°á»£c lÆ°u táº¡i:
# - Model cuá»‘i: Fedrag/FedE/x-model_*.bin
# - Models tá»«ng round: Fedrag/FedE/checkpoints/
```

### Downstream (RAG Evaluation)
```bash
# 1. CÃ i dependencies
cd Fedrag/RAGTest
pip install -r requirements.txt
pip install openai==1.55.3
pip install jury --no-deps
pip install gdown
pip install -U bitsandbytes

# 2. Download data vÃ  config
cd data
gdown https://drive.google.com/uc?id=1uiC3TfaUgbydukAAUgI9QR_Nj34WDIct -O test_corpus_backup.json
cd ..
rm config.toml
gdown https://drive.google.com/uc?id=1d-rlvn0IHeG9NRt-KRKgrhdssFG_GCES -O config.toml

# 3. Convert model (náº¿u cáº§n)
cd embs
python change.py ../../FedE/x-model_*.bin

# 4. Cháº¡y evaluation
cd ..
python main_100_test.py --model="../FedE/x-model_*_converted"
```

---

## ğŸ”„ Pipeline Tá»•ng Quan

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    UPSTREAM PIPELINE                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 1. Chuáº©n bá»‹  â”‚ -> â”‚ 2. Training  â”‚ -> â”‚ 3. Save Modelâ”‚    â”‚
â”‚  â”‚   Data       â”‚    â”‚   (FL)        â”‚    â”‚   (.bin)     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MODEL CONVERSION                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 4. Convert   â”‚ -> â”‚ 5. Save      â”‚ -> â”‚ 6. Ready for â”‚    â”‚
â”‚  â”‚   .bin -> HF â”‚    â”‚   HF Format  â”‚    â”‚   Downstream â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DOWNSTREAM PIPELINE                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 7. Load      â”‚ -> â”‚ 8. Build     â”‚ -> â”‚ 9. Evaluate  â”‚    â”‚
â”‚  â”‚   Model      â”‚    â”‚   Index      â”‚    â”‚   RAG System  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ PHáº¦N 1: UPSTREAM - FEDERATED LEARNING TRAINING

### Má»¥c ÄÃ­ch
Fine-tune embedding models (nhÆ° BGE-base-en) sá»­ dá»¥ng federated learning vá»›i dá»¯ liá»‡u phÃ¢n tÃ¡n trÃªn nhiá»u clients.

### BÆ°á»›c 1: CÃ i Äáº·t Miniconda

```bash
# Táº£i vÃ  cÃ i Ä‘áº·t Miniconda
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh 

bash Miniconda3-latest-Linux-x86_64.sh

# KÃ­ch hoáº¡t Conda
source ~/.bashrc

# Kiá»ƒm tra phiÃªn báº£n Conda
conda --version
```

### BÆ°á»›c 2: Táº¡o Conda Environment

```bash
# Cháº¥p nháº­n terms of service
conda tos accept --override-channels --channel https://repo.anaconda.com/pkgs/main
conda tos accept --override-channels --channel https://repo.anaconda.com/pkgs/r

# Táº¡o environment má»›i
conda create -n fedrag python=3.11 -y

# Khá»Ÿi táº¡o conda
conda init bash
source ~/.bashrc

# KÃ­ch hoáº¡t environment
conda activate fedrag
```

### BÆ°á»›c 3: Clone Repository vÃ  CÃ i Äáº·t Dependencies

```bash
# Clone repository
git clone https://github.com/Sukemcute/Fedrag.git

# Di chuyá»ƒn vÃ o thÆ° má»¥c FedE
cd Fedrag/FedE

# CÃ i Ä‘áº·t dependencies
pip install -r requirements.txt

# CÃ i Ä‘áº·t transformers vá»›i version cá»¥ thá»ƒ
pip install transformers==4.35.0

# CÃ i Ä‘áº·t numpy (version < 2)
pip install "numpy<2"
```

**YÃªu Cáº§u Pháº§n Cá»©ng:**
- GPU vá»›i Ã­t nháº¥t 80GB memory (khuyáº¿n nghá»‹: A40 hoáº·c tÆ°Æ¡ng Ä‘Æ°Æ¡ng)
- Batch size 16 yÃªu cáº§u GPU memory cao

### BÆ°á»›c 4: Chuáº©n Bá»‹ Dá»¯ Liá»‡u Training

#### 4.1. Download Dataset
Dataset training cÃ³ sáºµn táº¡i: [DocAILab/FedE4RAG_Dataset - train_data](https://huggingface.co/datasets/DocAILab/FedE4RAG_Dataset/tree/main/FEDE4FIN)

#### 4.2. Chá»n Dá»¯ Liá»‡u Training
Chá»‰nh sá»­a file `Fedrag/FedE/select_data.json` Ä‘á»ƒ chá»n dá»¯ liá»‡u training:
Sá»­a trong `Fedrag/FedE/flgo/benchmark/fedrag_classification/core.py`  -> sá»­a thÃ nh file dataset khÃ¡c lÃ  Ä‘Æ°á»£c.

```json
{
  "data_path": "/path/to/train_data/data_10000_random.json"
}
```

**CÃ¡c file training cÃ³ sáºµn:**
- `data_1000_random.json`
- `data_2000_random.json`
- `data_5000_random.json`
- `data_10000_random.json`
- `data_20000_random.json`
- `data_50000_random.json`

### BÆ°á»›c 5: Cáº¥u HÃ¬nh Training
(Cáº§n chá»‰nh thÃ´ng sá»‘ thÃ¬ cáº¥u hÃ¬nh vÃ o Ä‘Ã¢y, khÃ´ng thÃ¬ Ä‘á»ƒ nguyÃªn. )
Chá»‰nh sá»­a file `Fedrag/FedE/main.py`:

```python
import flgo
import flgo.algorithm.fedrag as fedrag
import os

os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'

# 1. Äáº·t tÃªn task (thÆ° má»¥c lÆ°u káº¿t quáº£)
task = './num5_alpha05'

# 2. Cáº¥u hÃ¬nh benchmark vÃ  partitioner
config = {
    'benchmark': {'name': 'flgo.benchmark.fedrag_classification'},
    'partitioner': {
        'name': 'IDPartitioner', 
        'para': {'num_clients': 5}  # Sá»‘ lÆ°á»£ng clients
    }
}

# 3. Generate task náº¿u chÆ°a tá»“n táº¡i
if not os.path.exists(task): 
    flgo.gen_task(config, task_path=task)

# 4. Khá»Ÿi táº¡o vÃ  cháº¡y federated learning
fedavg_runner = flgo.init(
    task=task, 
    algorithm=fedrag,
    option={
        'num_rounds': 25,      # Sá»‘ rounds federated learning
        'num_epochs': 1,        # Sá»‘ epochs má»—i round
        'gpu': 0,              # GPU ID
        'batch_size': 8,       # Batch size
        'learning_rate': 0.00001  # Learning rate
    }
)
fedavg_runner.run()
```

**CÃ¡c Tham Sá»‘ Quan Trá»ng:**
- `num_rounds`: Sá»‘ vÃ²ng federated learning (khuyáº¿n nghá»‹: 20-30)
- `num_clients`: Sá»‘ lÆ°á»£ng clients (vÃ­ dá»¥: 5)
- `batch_size`: Batch size (phá»¥ thuá»™c vÃ o GPU memory)
- `learning_rate`: Learning rate (khuyáº¿n nghá»‹: 1e-5)

### BÆ°á»›c 6: Cháº¡y Training

```bash
# Äáº£m báº£o Ä‘ang á»Ÿ trong thÆ° má»¥c Fedrag/FedE vÃ  Ä‘Ã£ activate conda environment
conda activate fedrag
cd Fedrag/FedE

# Cháº¡y training
bash run.sh
```

**LÆ°u Ã vá» `run.sh`:**
- File `run.sh` cÃ³ thá»ƒ Ä‘Æ°á»£c cáº¥u hÃ¬nh cho SLURM hoáº·c cháº¡y trá»±c tiáº¿p
- Náº¿u khÃ´ng dÃ¹ng SLURM, cÃ³ thá»ƒ cháº¡y trá»±c tiáº¿p: `python main.py`

### BÆ°á»›c 7: Káº¿t Quáº£ Training

Sau khi training xong, model sáº½ Ä‘Æ°á»£c lÆ°u tá»± Ä‘á»™ng:

**Output Files:**
- **Model cuá»‘i cÃ¹ng**: `Fedrag/FedE/x-model_{timestamp}.bin` 
  - Model nÃ y Ä‘Æ°á»£c lÆ°u sau khi hoÃ n thÃ nh táº¥t cáº£ cÃ¡c rounds
- **Models tá»«ng round**: `Fedrag/FedE/checkpoints/`
  - Má»—i model sau má»—i round sáº½ Ä‘Æ°á»£c lÆ°u trong thÆ° má»¥c `checkpoints`
  - Training thÆ°á»ng cháº¡y qua 25 rounds â†’ sáº½ cÃ³ 25 models trong checkpoints

**VÃ­ dá»¥ cáº¥u trÃºc:**
```
Fedrag/FedE/
  â”œâ”€â”€ x-model_2025-11-23_04-01-02.bin  â† Model cuá»‘i cÃ¹ng (sau 25 rounds)
  â”œâ”€â”€ checkpoints/
  â”‚   â”œâ”€â”€ round_0_model.bin
  â”‚   â”œâ”€â”€ round_1_model.bin
  â”‚   â”œâ”€â”€ ...
  â”‚   â””â”€â”€ round_24_model.bin
  â””â”€â”€ logs/
      â””â”€â”€ ...
```

**LÆ°u Ã:**
- Model Ä‘Æ°á»£c lÆ°u dÆ°á»›i dáº¡ng `.bin` file chá»©a `state_dict`
- Format: `{'model.embedding.weight': tensor(...), ...}`
- Model cuá»‘i cÃ¹ng (bÃªn ngoÃ i checkpoints) lÃ  model tá»‘t nháº¥t sau khi hoÃ n thÃ nh táº¥t cáº£ rounds
- Cáº§n convert sang HuggingFace format Ä‘á»ƒ sá»­ dá»¥ng trong downstream


---

## ğŸ“Š PHáº¦N 3: DOWNSTREAM - RAG EVALUATION

### BÆ°á»›c 8: Chuáº©n Bá»‹ MÃ´i TrÆ°á»ng Downstream

```bash
# Di chuyá»ƒn vÃ o thÆ° má»¥c RAGTest
cd Fedrag/RAGTest

# CÃ i Ä‘áº·t dependencies
pip install -r requirements.txt
pip install openai==1.55.3
pip install jury --no-deps
pip install gdown  # Äá»ƒ download files tá»« Google Drive
pip install -U bitsandbytes  # Cho quantization models
```

### BÆ°á»›c 9: Convert Model

### Má»¥c ÄÃ­ch
Convert model tá»« format `.bin` (state_dict) sang HuggingFace format Ä‘á»ƒ sá»­ dá»¥ng trong downstream.


Sá»­ dá»¥ng script `RAGTest/embs/change.py`:

```bash
cd RAGTest/embs

# CÃ¡ch 1: Convert tá»« file .bin  (dÃ¹ng cÃ¡i nÃ y)
python change.py ../FedE/x-model_2025-11-23_04-01-02.bin

# CÃ¡ch 2: Convert tá»« directory chá»©a .bin/.pt file
python change.py /path/to/model/directory

# CÃ¡ch 3: Chá»‰ Ä‘á»‹nh output directory
python change.py ../FedE/x-model.bin ./converted_model

# CÃ¡ch 4: Chá»‰ Ä‘á»‹nh base model khÃ¡c (náº¿u dÃ¹ng BGE-large)
python change.py ../FedE/x-model.bin ./converted_model "BAAI/bge-large-en-v1.5"
```

**Script sáº½:**
1. Load `state_dict` tá»« file `.bin`
2. XÃ³a prefix `'model.'` tá»« keys (náº¿u cÃ³)
3. Load base model (máº·c Ä‘á»‹nh: `BAAI/bge-base-en`) Ä‘á»ƒ láº¥y config vÃ  tokenizer
4. Load `state_dict` vÃ o model
5. Save model dÆ°á»›i dáº¡ng HuggingFace format (cÃ³ thá»ƒ load báº±ng `from_pretrained`)

**Output:**
```
converted_model/
  â”œâ”€â”€ config.json
  â”œâ”€â”€ pytorch_model.bin (hoáº·c model.safetensors)
  â”œâ”€â”€ tokenizer_config.json
  â”œâ”€â”€ vocab.txt
  â””â”€â”€ ...
```

**LÆ°u Ã:**
- Base model pháº£i khá»›p vá»›i model Ä‘Ã£ train (vÃ­ dá»¥: náº¿u train tá»« `BAAI/bge-base-en` thÃ¬ dÃ¹ng `BAAI/bge-base-en` lÃ m base)
- Náº¿u train tá»« `BAAI/bge-large-en-v1.5`, chá»‰ Ä‘á»‹nh base model tÆ°Æ¡ng á»©ng


### BÆ°á»›c 10: Download Dá»¯ Liá»‡u vÃ  Config

```bash
# Di chuyá»ƒn vÃ o thÆ° má»¥c data
cd data

# Download test corpus tá»« Google Drive
gdown https://drive.google.com/uc?id=1uiC3TfaUgbydukAAUgI9QR_Nj34WDIct -O test_corpus_backup.json

# Quay láº¡i thÆ° má»¥c RAGTest
cd ..

# XÃ³a file config.toml cÅ© (náº¿u cÃ³)
rm config.toml 

# Download config.toml má»›i tá»« Google Drive
gdown https://drive.google.com/uc?id=1d-rlvn0IHeG9NRt-KRKgrhdssFG_GCES -O config.toml
```

**LÆ°u Ã:**
- File `test_corpus_backup.json` sáº½ Ä‘Æ°á»£c lÆ°u trong `RAGTest/data/`
- File `config.toml` sáº½ Ä‘Æ°á»£c lÆ°u trong `RAGTest/`
- Äáº£m báº£o Ä‘Ã£ cÃ i `gdown` Ä‘á»ƒ download tá»« Google Drive

### BÆ°á»›c 11: Cáº¥u HÃ¬nh Downstream (TÃ¹y Chá»n)

Náº¿u cáº§n chá»‰nh sá»­a, má»Ÿ file `RAGTest/config.toml`:

```toml
[api_keys]
api_key = "sk-your-openai-api-key-here"  # Náº¿u dÃ¹ng OpenAI API
api_base = "https://api.openai.com/v1"
api_name = "gpt-4o-mini"  # hoáº·c "deepseek-r1:7b", "llama", etc.
auth_token = ""  # HuggingFace token náº¿u cáº§n

[settings]
llm = "gpt-4o-mini"  # LLM Ä‘á»ƒ generate response
embeddings = ""  # Äá»ƒ trá»‘ng, sáº½ dÃ¹ng --model argument
split_type = "sentence"  # "sentence" hoáº·c "word"
chunk_size = 2048
dataset = "json_download"  # TÃªn dataset
source_dir = "../wiki"  # ThÆ° má»¥c chá»©a documents
persist_dir = "storage"  # ThÆ° má»¥c lÆ°u index
retriever = "Vector"  # "Vector", "BM25", "Tree", etc.
postprocess_rerank = "long_context_reorder"
query_transform = "none"
n = 100  # Sá»‘ lÆ°á»£ng test samples
llamaIndexEvaluateModel = "Qwen/Qwen1.5-7B-Chat"  # Eval model (optional)
deepEvalEvaluateModel = "Qwen/Qwen1.5-7B-Chat"   # Eval model (optional)
```

**LÆ°u Ã:**
- File `config.toml` Ä‘Ã£ Ä‘Æ°á»£c download tá»« Google Drive, thÆ°á»ng Ä‘Ã£ Ä‘Æ°á»£c cáº¥u hÃ¬nh sáºµn
- Chá»‰ cáº§n chá»‰nh sá»­a náº¿u muá»‘n thay Ä‘á»•i cÃ¡c tham sá»‘

### BÆ°á»›c 12: Cháº¡y Evaluation
Cháº¡y test cáº£ 3 trÆ°á»ng há»£p Ä‘á»ƒ so sÃ¡nh viá»‡c dÃ¹ng model embedding cÃ³ sáºµn (BAAI/bge-base-en) vÃ  dÃ¹ng model embedding Ä‘Ã£ Ä‘Æ°á»£c train á»Ÿ Upstream.

**Sá»± khÃ¡c biá»‡t giá»¯a `main_100_test.py` vÃ  `main_response.py`?**
->  `main_100_test.py` chá»‰ test retrieval, `main_response.py` test cáº£ response generation vá»›i NLG metrics.

#### TrÆ°á»ng Há»£p 1: Test vá»›i Pretrained Model (Baseline)

```bash
# Äáº£m báº£o Ä‘ang á»Ÿ trong thÆ° má»¥c RAGTest
cd Fedrag/RAGTest

# Test vá»›i BGE-base-en (pretrained)
python main_100_test.py --model="BAAI/bge-base-en"

# Test vá»›i BGE-large
python main_100_test.py --model="BAAI/bge-large-en-v1.5"
```

#### TrÆ°á»ng Há»£p 2: Test vá»›i Fine-tuned Model (Tá»« Upstream)

**BÆ°á»›c 2.1: Convert Model (Náº¿u chÆ°a convert)**

```bash
# Convert model tá»« upstream sang HuggingFace format
cd Fedrag/RAGTest/embs
python change.py ../../FedE/x-model_2025-11-23_04-01-02.bin
```

**BÆ°á»›c 2.2: Test vá»›i Model ÄÃ£ Convert**

```bash
# Quay láº¡i thÆ° má»¥c RAGTest
cd Fedrag/RAGTest

# Test vá»›i model Ä‘Ã£ convert tá»« upstream
python main_100_test.py --model="../FedE/x-model_2025-11-23_04-01-02_converted"

# Hoáº·c náº¿u Ä‘Ã£ convert vÃ o thÆ° má»¥c khÃ¡c
python main_100_test.py --model="./converted_model"
```

#### TrÆ°á»ng Há»£p 3: Test vá»›i Response Generation

```bash
# Test vá»›i response generation (bao gá»“m cáº£ NLG metrics)
cd Fedrag/RAGTest
python main_response.py --model="../FedE/x-model_2025-11-23_04-01-02_converted"
```

#### TrÆ°á»ng Há»£p 4: Batch Testing (Nhiá»u Models)  (khÃ´ng cáº§n test cÃ¡i nÃ y)

Sá»­a file `Fedrag/RAGTest/bash.sh` hoáº·c `Fedrag/RAGTest/bash1.sh`:

```bash
# bash.sh - Test retrieval metrics
python main_100_test.py --model="/path/to/model1" > log1.log
python main_100_test.py --model="/path/to/model2" > log2.log

# bash1.sh - Test vá»›i response generation
python main_response.py --model="/path/to/model1" > log1.log
python main_response.py --model="/path/to/model2" > log2.log
```

Cháº¡y:
```bash
cd Fedrag/RAGTest
bash bash.sh      # Chá»‰ test retrieval
bash bash1.sh     # Test vá»›i response generation
```

### BÆ°á»›c 13: Káº¿t Quáº£ Evaluation

**Output Files:**

1. **`0407_0318+0322.txt`** (hoáº·c tÆ°Æ¡ng tá»±): File chá»©a káº¿t quáº£ evaluation
   - TRT metrics (Hit@k, Recall@k, Precision@k, F1, EM, MRR, MAP, NDCG)
   - NLG metrics averages (ROUGE, METEOR, CHRF, WER, CER, Perplexity)
   - Metrics theo categories (domain-relevant, metrics-generated, novel-generated)

2. **`storage-{dataset}-{model}-{config}/`**: ThÆ° má»¥c chá»©a vector index
   - CÃ³ thá»ƒ tÃ¡i sá»­ dá»¥ng Ä‘á»ƒ trÃ¡nh rebuild index

3. **Log files** (khi dÃ¹ng bash scripts):
   - `logs_test_100/`: ThÆ° má»¥c chá»©a log files

**VÃ­ dá»¥ Output:**
```
TRT ----------------------------------------------------------------------------------------------------
n: 50
F1: 0.6666666666666667
em: 0.6
mrr: 0.7933333333333334
hit1: 0.82
hit10: 0.88
MAP: 0.78
NDCG: 0.815355919212455

NLG Evaluation Metrics Averages:
cos_1: 0.7200
recall_1: 0.6167
precision: 0.7200
chrf_pp: 0.2243
perplexity: 178.7065
rouge_rouge1: 0.2297
rouge_rouge2: 0.1071
...

NLG Evaluation Metrics Averages by Category:
--- Category: domain-relevant (Evaluations: 17) ---
  hit_1: 0.5294
  rouge_rouge1: 0.3365
  ...
```

---

## ğŸ“ˆ CÃ¡c Metrics ÄÆ°á»£c TÃ­nh

### Retrieval Metrics (TRT - Text Retrieval Task)
- **Hit@k**: Hit rate táº¡i top-k (k=1,3,5,10)
- **Recall@k**: Recall táº¡i top-k
- **Precision@k**: Precision táº¡i top-k
- **F1**: F1 score
- **EM**: Exact Match
- **MRR**: Mean Reciprocal Rank
- **MAP**: Mean Average Precision
- **NDCG**: Normalized Discounted Cumulative Gain

### Generation Metrics (NLG - Natural Language Generation)
- **ROUGE**: ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-Lsum (vá»›i P, R)
- **METEOR**: METEOR score
- **CHRF/CHRF++**: CHRF score
- **WER/CER**: Word/Character Error Rate
- **Perplexity**: Perplexity
- **Cosine Similarity**: cos_1, cos_3, cos_5, cos_10

### Categories
Káº¿t quáº£ Ä‘Æ°á»£c phÃ¢n loáº¡i theo `question_type`:
- **domain-relevant**: CÃ¢u há»i vá» domain cá»¥ thá»ƒ
- **metrics-generated**: CÃ¢u há»i vá» sá»‘ liá»‡u/metrics
- **novel-generated**: CÃ¢u há»i má»›i/tá»•ng há»£p

---

## ğŸ› Troubleshooting

### Upstream Issues

**Lá»—i: Out of Memory**
- Giáº£m `batch_size` trong `main.py`
- Sá»­ dá»¥ng GPU cÃ³ memory lá»›n hÆ¡n
- Giáº£m sá»‘ lÆ°á»£ng clients

**Lá»—i: Model khÃ´ng save**
- Kiá»ƒm tra path trong `fedrag.py` (dÃ²ng 45, 56)
- Äáº£m báº£o cÃ³ quyá»n ghi vÃ o thÆ° má»¥c

### Conversion Issues

**Lá»—i: FileNotFoundError**
- Kiá»ƒm tra Ä‘Æ°á»ng dáº«n model file
- Äáº£m báº£o file `.bin` tá»“n táº¡i
- Kiá»ƒm tra quyá»n Ä‘á»c file

**Lá»—i: Missing keys khi load state_dict**
- ThÆ°á»ng lÃ  bÃ¬nh thÆ°á»ng (má»™t sá»‘ keys cÃ³ thá»ƒ missing)
- Kiá»ƒm tra base model cÃ³ khá»›p vá»›i model Ä‘Ã£ train khÃ´ng

### Downstream Issues

**Lá»—i: Model khÃ´ng load Ä‘Æ°á»£c**
- Kiá»ƒm tra Ä‘Æ°á»ng dáº«n model
- Kiá»ƒm tra HuggingFace token náº¿u dÃ¹ng private model
- Kiá»ƒm tra GPU memory

**Lá»—i: Device mismatch (CPU vs CUDA)**
- Äáº£m báº£o embedding model vÃ  LLM Ä‘á»u trÃªn cÃ¹ng device
- Xem `RAGTest/embs/embedding.py` vÃ  `RAGTest/llms/huggingface_model.py`


**Lá»—i: Dataset khÃ´ng tÃ¬m tháº¥y**
- Kiá»ƒm tra `dataset` name trong config
- Kiá»ƒm tra file trong `data/` folder
- Kiá»ƒm tra `source_dir` path

---

## ğŸ“ Checklist HoÃ n Chá»‰nh

### Upstream
- [ ] CÃ i Ä‘áº·t Miniconda
- [ ] Táº¡o conda environment `fedrag` vá»›i Python 3.11
- [ ] Clone repository `https://github.com/Sukemcute/Fedrag.git`
- [ ] CÃ i Ä‘áº·t dependencies (`requirements.txt`, `transformers==4.35.0`, `numpy<2`)
- [ ] Download training dataset
- [ ] Chá»‰nh sá»­a `select_data.json` (náº¿u cáº§n)
- [ ] Cáº¥u hÃ¬nh `main.py` (num_rounds, num_clients, batch_size, learning_rate)
- [ ] Cháº¡y training (`bash run.sh`)
- [ ] Kiá»ƒm tra model output:
  - [ ] Model cuá»‘i cÃ¹ng: `Fedrag/FedE/x-model_*.bin`
  - [ ] Models tá»«ng round: `Fedrag/FedE/checkpoints/`

### Conversion
- [ ] XÃ¡c Ä‘á»‹nh base model (BGE-base hoáº·c BGE-large)
- [ ] Cháº¡y `change.py` Ä‘á»ƒ convert model tá»« `.bin` sang HuggingFace format
- [ ] Kiá»ƒm tra output directory cÃ³ Ä‘áº§y Ä‘á»§ files (config.json, pytorch_model.bin, tokenizer files)

### Downstream
- [ ] CÃ i Ä‘áº·t dependencies (`requirements.txt`, `openai==1.55.3`, `jury`, `gdown`, `bitsandbytes`)
- [ ] Download `test_corpus_backup.json` tá»« Google Drive vÃ o `RAGTest/data/`
- [ ] Download `config.toml` tá»« Google Drive vÃ o `RAGTest/`
- [ ] Test vá»›i pretrained model (baseline)
- [ ] Convert model tá»« upstream (náº¿u chÆ°a convert)
- [ ] Test vá»›i fine-tuned model
- [ ] So sÃ¡nh káº¿t quáº£
- [ ] PhÃ¢n tÃ­ch metrics theo categories

---

## ğŸš€ Next Steps

1. **So SÃ¡nh Models**: Test nhiá»u models vÃ  so sÃ¡nh káº¿t quáº£
2. **Fine-tune Hyperparameters**: Äiá»u chá»‰nh `chunk_size`, `retriever`, `postprocess_rerank`
3. **Enable Advanced Metrics**: Uncomment cÃ¡c metrics trong code (Llama_, DeepEval_, UpTrain_)
4. **Custom Evaluation**: Táº¡o custom metrics hoáº·c tÃ­ch há»£p frameworks khÃ¡c
5. **Export Results**: Export káº¿t quáº£ sang CSV/JSON Ä‘á»ƒ phÃ¢n tÃ­ch

---

## ğŸ“š TÃ i Liá»‡u Tham Kháº£o

- **README.md**: Tá»•ng quan vá» project
- **DOWNSTREAM_GUIDE.md**: HÆ°á»›ng dáº«n chi tiáº¿t downstream
- **config.toml**: Táº¥t cáº£ cÃ¡c tham sá»‘ cáº¥u hÃ¬nh
- **Dataset**: [DocAILab/FedE4RAG_Dataset](https://huggingface.co/datasets/DocAILab/FedE4RAG_Dataset)

---

*Cáº­p nháº­t láº§n cuá»‘i: 2025-01-XX*

